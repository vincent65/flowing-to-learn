# arXiv:2506.05350v1 [cs.CV] 5 Jun 2025

## Contrastive Flow Matching

## George Stoica^12 † Vivek Ramanujan^2 ♢ Xiang Fan^2 ♢

## Ali Farhadi^2 Ranjay Krishna^2 Judy Hoffman^1

(^1) Georgia Tech (^2) University of Washington

## †Correspondence to:gstoica3@gatech.edu ♢Equal Contribution

```
Figure 1.Training with Contrastive Flow-Matching (∆FM) improves natural image generation.(leftis baseline,rightis with∆FM)
Here we show comparisons between images generated by diffusion models trained on ImageNet-1k ( 512 × 512 ). Each pair of images
is generated with the same class and initial noise to ensure similar image structure for comparability. We see that our∆FMobjective
encourages significantly more coherent images and improves the consistency of global structure.
```
## Abstract

```
Unconditional flow-matching trains diffusion models to
transport samples from a source distribution to a target dis-
tribution by enforcing that the flows between sample pairs
are unique. However, in conditional settings (e.g., class-
conditioned models), this uniqueness is no longer guaran-
teed—flows from different conditions may overlap, leading
to more ambiguous generations. We introduce Contrastive
Flow Matching, an extension to the flow matching objec-
tive that explicitly enforces uniqueness across all condi-
tional flows, enhancing condition separation. Our approach
adds a contrastive objective that maximizes dissimilarities
between predicted flows from arbitrary sample pairs. We
validate Contrastive Flow Matching by conducting exten-
sive experiments across varying model architectures on both
class-conditioned (ImageNet-1k) and text-to-image (CC3M)
benchmarks. Notably, we find that training models with Con-
trastive Flow Matching (1) improves training speed by a
factor of up to 9 ×, (2) requires up to 5 ×fewer de-noising
steps and (3) lowers FID by up to 8. 9 compared to training
the same models with flow matching. We release our code
at: https://github.com/gstoica27/DeltaFM.git.
```
## 1. Introduction

```
Flow matching for generative modeling trains continuous
normalizing flows by regressing ideal probability flow fields
between a base (noise) distribution and the data distribu-
tion [ 25 ]. This approach enables straight-line generative
trajectories and has demonstrated competitive image synthe-
sis quality. However, for conditional generation (e.g., class-
conditional image generation), vanilla flow matching models
often produce outputs that resemble an “average” of the
possible images for a given condition, rather than a distinct
mode of that condition. In essence, the model may collapse
multiple diverse outputs into a single trajectory, yielding
samples that lack the expected specificity and diversity for
each condition [ 29 , 44 ]. By contrast, an unconditional flow
model—tasked with covering the entire data distribution
without any conditioning—implicitly learns more varied
flows for different modes of the data. Existing conditional
flow matching formulationsdo not enforce the flows to differ
across conditions, which can lead to this averaging effect
and suboptimal generation fidelity.
To address these limitations and improve generation qual-
ity, recent work has explored enhancements to structure the
generator’s representations and also proposed inference-time
guidance strategies. For example, one approach is to in-
corporate aREPresentation Alignment(REPA) objective to
```

```
Figure 2.∆FMyields more discriminative and higher quality trajectories. (left)shows the result of standard flow-matching, where
flows are straight but end up overlapping for similar class distributions.(right)shows how the addition of the∆FMobjective results in
more distinct flows, resulting in images which are more representative of their respective classes.
```
structure the representations at an intermediate layer with
those from a high-quality pretrained vision encoder [ 44 ]. By
using feature embeddings from a DINO self-supervised vi-
sion transformer [ 5 , 31 ], the generative model’s hidden states
are guided toward semantically meaningful directions. This
representational alignment provides an additional learning
signal that has been shown to improve both training conver-
gence and final image fidelity, albeit at the cost of requiring
an external pretrained encoder and an auxiliary loss term.
Another popular technique isclassifier-free guidance(CFG)
for conditional generation [ 18 ], which involves jointly train-
ing the model in unconditional and conditional modes (often
by randomly dropping the condition during training). At in-
ference time, CFG performs two forward passes—one with
the conditioning input and one without—and then extrapo-
lates between the two outputs to push the sample closer to
the conditional target [ 18 , 29 ]. While CFG can significantly
enhance image detail and adherence to the prompt or class la-
bel, it doubles the sampling cost and complicates training by
necessitating an implicit unconditional generator alongside
the conditional ones [11, 11, 20].

```
We proposeContrastive Flow Matching(∆FM), a new
approach that augments the flow matching objective with an
auxiliary contrastive learning objective.∆FMencourages
more diverse and distinct conditional generations. It applies
a contrastive loss on the flow vectors (or representations) of
samples within each training batch, encouraging the model
to produce dissimilar flows for different conditioning inputs.
Intuitively, this loss penalizes the model if two samples with
different conditions yield similar flow dynamics, thereby
explicitly discouraging the collapse of multiple conditions
onto a single “average” generative trajectory. As a result,
given a particular condition, the model learns to generate
a unique flow through latent space that is characteristic of
that condition alone, leading to more varied and condition-
specific outputs. Importantly, this contrastive augmentation
iscomplementaryto existing methods. It can be applied
```
```
along with REPA, further ensuring that flows not only align
with pretrained features but also remain distinct across condi-
tions. Likewise, it is compatible with classifier-free guidance
at sampling time, allowing one to combine its benefits with
CFG for even stronger conditional signal amplification.
```
```
Inspired by contrastive training objectives,∆FMapplies
a pairwise loss term between samples in a training batch:
for each positive sample from the batch, we randomly sam-
ple a negative counterpart. We then encourage the model
to not only learn the flow towards the positive sample but
also to learn the flow away from the negative sample. This
is achieved by adding a contrastive loss to the flow match-
ing objective, which promotes class separability throughout
the flow. Our method is simple to implement and can be
easily integrated into existing diffusion models without any
additional data and with minimal computational overhead.
```
```
We validate the advantages of∆FMthrough (1) exten-
sive experiments on conditional image generation using Im-
ageNet images across multiple SiT [ 29 ] model scales and
training frameworks [ 29 , 44 ], and (2) text-to-image experi-
ments on the CC3M [ 37 ] with the MMDiT [ 14 ] architecture.
Thanks to contrastive flows,∆FMconsistently outperforms
traditional diffusion flow matching in quality and diversity
metrics, achieving up to an 8. 9 -point reduction in FID-50K
on ImageNet, and 5 -point reduction in FID on the whole
CC3M validation set. It is also compatible with recent sig-
nificant improvements in the diffusion objective, such as
Representation Alignment (REPA) [ 44 ]. By encouraging
class separability,∆FMis able to efficiently reach a given
image quality with 5 ×fewersampling steps than a baseline
Flow Matching model, translating directly to faster genera-
tion. It also enhances training efficiency by up to 9 ×. Finally,
∆FMstacks with classifier-free guidance, lowering FID by
5.7%compared to flow matching models.
```

## 2. Related works

Our work lies in the domain of image generative models,
primarily diffusion and flow matching models. We augment
flow-matching with a contrastive learning objective to pro-
vide an alternative solution to classifier free guidance.
Generative modeling has rapidly advanced through two
primary paradigms: diffusion-based methods [ 19 , 39 ] and
flow matching [ 25 ].Denoising diffusion modelstypically
rely on stochastic differential equations (SDEs) and score-
based learning to iteratively add and remove noise [ 19 ]. De-
noising diffusion implicit models (DDIMs) [ 39 ] reduce this
sampling complexity by removing non-determinism in the
reverse process, while progressive distillation [ 34 ] further
accelerates inference by shortening the denoising chain. Ad-
vanced ODE solvers [ 6 ] and distillation methods [ 41 ] have
also enhanced sampling efficiency. Despite their success,
diffusion models can be slow at inference due to iterative
denoising [19].
Flow matching[ 6 ] has been designed to reduce infer-
ence steps. It directly parameterizes continuous-time trans-
port dynamics for more efficient sampling. Probability flow
ODEs [ 25 , 39 ] learn an explicit transport map between data
and latent distributions. Unlike diffusion models, it bypasses
separate score estimation and stochastic noise, which re-
duces function evaluations and tends to improve training
convergence [ 6 ]. A common type of flow matching algo-
rithm popularized recently is the rectified flow [ 26 ], which
refines probability flow ODEs through direct optimal trans-
port learning, improving numerical stability and sampling
speed. This approach mitigates the high computational bur-
den of diffusion sampling while maintaining high-fidelity
image generation with fewer integration steps.
Since both diffusion and flow matching models are trained
to match the target distribution of real images, they often
produce ‘averaged’ samples that lack the sharp details and
strong conditional fidelity [ 17 ]. Regardless of how much
these models speed up, they often need to be invoked mul-
tiple times with unique seed noise to find a high-fidelity
sample. In response,guidance techniqueshave been in-
troduced to substantially promote high-fidelity synthesis.
Classifier guidance [ 12 ], classifier-free guidance [ 17 ], en-
ergy guidance [ 8 , 27 , 40 , 45 ], and more advanced meth-
ods [ 9 , 20 , 21 , 23 , 38 ] improve fidelity and controllabil-
ity, without requiring multiple invocations. Although they
achieve remarkable performance, they typically still require
additional computational overhead. CFG requires calling
sampling from a second ‘unconditional’ generation and guid-
ing the ‘conditional’ generation away from the unconditional
variant [ 28 , 42 , 43 , 46 ]. We adapt the flow matching objec-
tive with a contrastive loss between the transport vectors
within a batch. By doing so, we achieve the same benefits of
CFG, without the additional overhead of needing to train an
unconditional generator or using one during inference.

```
Contrastive learning was originally proposed for face
recognition [ 36 ], where it was designed to encourage a mar-
gin between positive and negative face pairs. In generative
adversarial networks (GANs), it has been applied to improve
sample quality by structuring latent representations [ 4 ]. How-
ever, to the best of our knowledge, it has not been explored
in the context of visual diffusion or flow matching models.
We incorporate this contrastive objective to demonstrate its
utility in speeding up training and inference of flow-based
generative models.
```
## 3. Background and motivation

```
We focus on flow matching models [ 25 ] due to its rising
popularity as an effective training paradigm for generative
models [ 1 , 2 , 24 ]. In this section, we provide a brief overview
of flow matching through the perspective of stochastic inter-
polants [2, 29], as it pertains to our work.
Preliminaries.Letp(x)be an arbitrary distribution defined
on the reals, and letN(0,I)be a Gaussian noise distribution.
The objective of flow matching is to learn a transport between
the two distributions. That is, given an arbitraryε∼N(0,I),
a flow matching model gradually transformsεover time into
anxˆthat is part ofp(x). Stochastic interpolants [ 2 , 29 ] define
this transformation as a time-dependent stochastic process,
where transformation steps are summarized as follows,
```
```
ˆxt=αtxˆ+σtε (1)
```
```
whereαt andσt are decreasing and increasing time-
dependent functions respectively defined ont∈[0,T], such
thatαT=σ 0 = 1andα 0 =σT= 0. While theoretically,
αt,σtneed not be linear, linear complexity is often sufficient
to obtain strong diffusion models [25, 29, 44].
Flow matching.Given such a process, flow matching mod-
els learn to transport between noise top(x)by estimating a
velocity field over an probability flow ordinary differential
equation (PF ODE),dxt=v(xt,t)dt, whose distribution at
timetis the marginalpt(x). This velocity is given by the
expectations ofxˆandεconditioned onxt,
```
```
v(xt,t) = ̇αtE[ˆx|xt=x] + ̇σtE[ε|xt=x], (2)
```
```
whereα ̇t,σ ̇tare the time-based derivatives ofαtandσt
respectively. Since,ˆxandεare arbitrary samples from their
respective distributions,v(xt,t)is expected “direction” of
all transport paths between noise andp(x)that pass through
xtatt. While the optimalv(xt,t)is intractable, it can be
approximated with a flow-modelvθ(xt,t), by minimizing
the training objective:
```
```
L(FM)(θ) =E
```
#### 

```
||vθ(xt,t)−( ̇αtxˆ+ ̇σtε)||^2
```
#### 

#### (3)

```
Key to understanding the properties of flow matching is
the concept of flow uniqueness [ 25 ]. That is, flows fol-
lowing the well-defined ODE cannot intersect atanytime
```

```
t∈[0,T). As such, flow models can iteratively refine
unique-discriminative features relevant to anyx∼p(x)in
eachxt, leading to more efficient and accurate diffusion
paths compared to other training paradigms [25].
Conditional flow matching.Commonly,p(x)may be a
marginal distribution over several class-conditional distribu-
tions (e.g., the classes of ImageNet [ 33 ]). Training models
in such cases is nearly identical to standard flow-matching,
except that flows are further conditioned on the target distri-
bution class:
```
```
L
(FM)
cond(θ) =E
```
#### 

```
||vθ(xt,t,y)−( ̇αtˆx+ ̇σtε)||^2
```
#### 

#### , (4)

whereˆx∼p(x|y). Resultant models have the desirable
trait of being more controllable: their generated outputs can
be tailored to their respective input conditions. However,
this comes at the notable cost of flow-uniqueness. Specif-
ically these models only generate unique flows compared
to otherswithinthe same class-condition, not necessarily
acrossclasses. This inhibitsxt’s from storing important
class-specific features and leads to poorer quality genera-
tions. Second, the conditional flow matching objective trains
models without knowledge of the distributional spread from
other class-conditions, leading to flows that may generate
ambiguous outputs when conditional distributions overlap

. This increases the likelihood of ambiguous generations
that form a mixture between different conditions, restricting
model capabilities. We study these effects in Section 5.

## 4. Contrastive Flow-Matching

We introduce Contrastive Flow Matching (∆FM), a novel
approach designed to address the challenges of learning effi-
cient class-distinct flow representations in conditional gen-
erative models. Standard conditional flow matching (FM)
models tend to produce flow trajectories that align across dif-
ferent samples, leading to reduced class separability.∆FM
extends the FM objective by incorporating a contrastive
regularization term, which explicitly discourages alignment
between the learned flow trajectories of distinct samples.
Ingredients.Letx ̃∼p(x|y ̃)denote a sample drawn from
the data distribution conditioned on an arbitrary classy ̃, and
let ̃ε∼N(0,I)represent an independent noise sample. To
ensure that the contrastive objective captures distinct flow
trajectories, we impose the conditionsx ̸̃= ˆxand ̃ε̸=ε,
wherey ̃may or may not be equal toy. Importantly, we
do not assume the existence of a time stept∈[0,T]such
thatxt=αtx ̃+σt ̃ε. Consequently,x ̃and ̃εrepresent truly
independent flow trajectories in comparison toˆxandε.

The contrastive regularization.Givenvθ(xt,t,y)and an
arbitraryx, ̃ ̃εsample pair, the contrastive objective aims to
maximizethe dissimilarity between the estimated flow of
vθ(xt,t,y)fromεtoˆx, and the independent flow produced

```
by ̃x, ̃ε. We achieve this by maximizing the quantity,
```
```
E
```
#### 

```
||vθ(xt,t,y)−( ̇αtx ̃+ ̇σt ̃ε)||^2
```
#### 

#### . (5)

```
Since ̃xis drawn from the marginalp(x)rather thanp(x|y),
Equation 5 trains flow matching models to produce flows
that areunconditionallyunique.
Putting it all together. We now define contrastive flow
matching as follows,
```
```
L(∆FM)(θ) = E
```
#### "

```
||vθ(xt,t,y)−( ̇αtxˆ+ ̇σtε)||^2
−λ||vθ(xt,t,y)−( ̇αtx ̃+ ̇σt ̃ε)||^2
```
#### #

#### (6)

```
whereλ∈[0,1)is a fixed hyperparameter that controls
the strength of the contrastive regularization. Thus,∆FM
simultaneously encourages flow matching models to esti-
mate effective transports from noise to corresponding class-
conditional distributions (the flow matching objective), while
enforcing each to be discriminativeacrossclasses (con-
trastive regularization). Note that∆FMcan be thought
of as a generalization of flow matching, as∆FMreduces
to FM whenλ= 0. We study the effects of varyingλin
Section 5.5.
Implementation.Contrastive flow matching (∆FM) is eas-
ily integrated into any flow matching training loop, with
minimal overhead. Algorithm 1 illustrates the implemen-
tation of an arbitrary batch step, where navy text marks
additions to the standard flow matching objective. Thus,
∆FMsolely depends on the information already available
to the flow matching objective at each batch step, without
computing any additional forward steps. Furthermore,∆FM
seamlessly folds into flow matching training regimes, mak-
ing it a “plug-and-play” objective for existing setups.
```
```
Algorithm 1Contrastive Flow-Matching Batch Step
1:Input: A modelvθ, batch of N flow examples
F={(x 1 ,y 1 ,ε 1 ),...,(xN,yN,εN)}where(xi,yi)∼
p(x,y)andεi∼N(0,I),βlearning rate,λ= 0. 05.
2:Output:Updated model parametersθ
3:L(θ) = 0
4:foriin range(N)do
5: t∼U(0,1),xt=αtxi+σtεi
6: sample( ̃x,y, ̃ ̃ε)∼F,s.t.( ̃x,y, ̃ ̃ε)̸= (xi,yi,εi)
7: vˆ=v(xt,t,yi),v= ̇αtxi+ ̇σtε, ̃v= ̇αtx ̃+ ̇σtε ̃
8: L(θ)+ =||vˆ−v||^2 −λ||vˆ− ̃v||^2
9:end for
10:θ←θ−Nβ∇θL(θ)
```
```
Discussion. Figure 3 illustrates the effects of contrastive
flow matching compared to flow matching. The figure shows
the resultant flows after training a small diffusion model in a
simple toy-setting. Specifically, we create a two-dimensional
```

```
Figure 3.Contrastive Flow-Matching intrinsically separates
flows between classes.We train a small three layer MLP flow-
matching model to transport between a two dimensional multivari-
ate noise distribution (violet) and two independent blue and orange
class distributions respectively. The class distributions are designed
to have∼50%overlap, and we plot the learned class-conditioned
flows between noise samples and each respective class distribution
using class colors. Top: Flow-matching models learn overlapping
transports between distributions, generating outputs that lie in am-
biguous regions between the two classes. Bottom: Contrastive
flow-matching models have significantly more discriminative flows,
generating class-coherent samples while reducing ambiguity.
```
violet gaussian noise distribution and two independent two-
dimensional class distributions (in blue and orange respec-
tively) such that the latter distributions have≈50%overlap.
Samples from each distribution are represented as “dots”,
with those in the target distributions colored according to the
gaussiankernel-density estimatebetween samples from each
class in their respective region. We observe that training
the model with flow matching (top) create flows with large
degrees of overlap between classes, generating samples with
lower class-distinction. In contrast, training the same model
with contrastive flow matching (bottom) yields trajectories
that are significantly more diverse across classes, while also
generating samples which capture distinct features of each
respective class.

## 5. Experiments

We validate contrastive flow-matching (∆FM) through ex-
tensive experiments across various model, training and
benchmark configurations. Overall, models trained with
∆FMconsistently outperform flow-matching (FM) models

```
acrossallsettings.
Datasets.We conduct both class-conditioned and text-to-
image experiments. We use ImageNet-1k [ 10 ] processed
at both ( 256 × 256 ) and ( 512 × 512 ) resolutions for our
class-conditioned experiments, and follow the data prepro-
cessing procedure of ADM [ 12 ] We then follow [ 44 ] and
encode each image using the Stable Diffusion VAE [ 32 ] into
a tensorz∈R^32 ×^32 ×^4. For text-to-image (t2i), we use the
Conceptual Captions 3M (CC3M) dataset [ 37 ] processed
at ( 256 × 256 ) resolution and follow the data processing
procedure of [ 3 ]. We train all models by strictly following
the setup in [ 44 ], and use a batch size of 256 unless other-
wise specified. We do not alter the training conditions to
be favorable to∆FM, and we always setλ= 0. 05 when
applicable.
Measurements.We report five quantitative metrics through-
out our experiments. We report Fr ́echet inception distance
(FID) [ 16 ], inception score (IS) [ 35 ], sFID [ 30 ], precision
(Prec.) and recall (Rec.) [ 22 ] using 50,000 samples for our
class-conditioned experiments. Similarly, we report FID
over the whole validation set in the text-to-image setting.
We use the SDE Euler-Maruyama sampler withwt=σtfor
all experiments, and set the number of function evaluations
(NFE) to 50 unless otherwise specified.
```
### 5.1. Contrastive Flow-Matching Improves SiT

```
Implementation details. We train on the state-of-the-art
SiT [ 29 ] model architecture, using both SiT-B/2 and SiT-
XL/2.
Results. Table 1 summarizes our results. Overall,∆FM
dramatically improves over flow-matching in nearly all met-
rics (only matching the flow-matching SiT-XL/2 model in
recall). Notably, employing∆FMwith SiT-B/2 lowers FID
by over 8 compared to flow-matching at both ImageNet res-
olutions, highlighting the strength of∆FMin smaller model
scales. Similarly,∆FMis robust to larger model scales and
outperforms FM by over 3.2 FID when using SiT-XL/2.
```
### 5.2. REPA is complementary

```
REPresentation Alignment (REPA) [ 44 ] is a recently intro-
duced training framework that rapidly improves diffusion
model performance by strengthening its intermediate rep-
resentations. Specifically, REPA distills the encodings of
foundation vision encoders (e.g., DiNOv2 [ 5 ]) into the hid-
den states of diffusion models through the use of an aux-
illiary objective. Notably, REPA can improve the training
speed of vanilla SiT models by over 17. 5 ×, while further im-
proving their performances [ 44 ].∆FMis easily integrated
into REPA and only requires replacing the flow-matching
objective.
Implementation details.We apply REPA on the same SiT
models as in Section 5.1, and use the distillation process de-
```

```
Metrics
Model FID↓ IS↑ sFID↓ Prec.↑ Rec.↑
SiT-B/2 42.28 38.04 11.35 0.5 0.
+ Using∆FM 33.39 43.44 5.67 0.53 0.
SiT-XL/2 20.01 74.15 8.45 0.63 0.
+ Using∆FM 16.32 78.07 5.08 0.66 0.
```
(a)ImageNet-1k (256x256) Results.∆FMsignificantly outperforms flow-
matching models across nearly all metrics, and matches Recall on SiT-XL/2.

```
Metrics
Model FID↓ IS↑ sFID↓ Prec.↑ Rec.↑
SiT-B/2 50.26 33.58 14.88 0.57 0.
+ Using∆FM 41.59 38.20 6.13 0.62 0.
SiT-XL/2 22.98 70.14 10.71 0.73 0.
+ Using∆FM 19.67 72.58 4.98 0.76 0.
```
(b)ImageNet-1k (512x512) Results.Models trained with∆FMeither
substantially outperform or match their flow-matching counterparts in all
metrics.

Table 1. SiT [ 29 ] results on ImageNet-1k ( 256 × 256 ; a) and
( 512 × 512 ; b). We train all models for 400K iterations following
[ 44 ]. All metrics are measured with the SDE Euler-Maruyama
sampler with NFE=50 and without classifier guidance. We use
λ= 0. 05 for all models trained with∆FMand do not change any
other hyperparameters.↑indicates that higher values are better,
with↓denoting the opposite.

fined by [ 44 ] exactly. Specifically, we use distill DiNOv2 [ 5 ]
ViT-B [ 13 ] features into the 4th layer of the SiT-B/2, and the
8th layer of the SiT-XL/2, and mirror their hyperparameter
setup.
Results.We report results in Table 2. Similar to Section 5.1,
∆FMsubstantially improves REPA models by as much as
6.81 FID, and consistently improves flow-matching with
model scale. This highlights the versatility of the contrastive
flow-matching objective as a broadly applicable criterion for
diffusion model.

### 5.3. Extending to text-to-image generation

```
Implementation Details.We train models with the pop-
ular MMDiT [ 14 ] architecture from scratch on the CC3M
dataset [ 37 ] for 400K iterations. For faster training, we
pair each model with REPA, and follow the recommended
training protocol of [44].
Results.Table 3 shows our results.∆FMimproves over the
flow matching baseline by 5 FID, highlighting its seamless
transferability to the broader text-to-image setting. We show
qualitative results in Appendix A.
```
### 5.4. CFG stacks with contrastive flow matching

```
Contrastive flow matching offers advantages of Classifier-
Free Guidance (CFG), without incurring additional computa-
```
```
Metrics
Model FID↓ IS↑ sFID↓ Prec.↑ Rec.↑
REPA SiT-B/2 27.33 61.60 11.70 0.57 0.
+ Using∆FM 20.52 69.71 5.47 0.61 0.
REPA SiT-XL/2 11.14 115.83 8.25 0.67 0.
+ Using∆FM 7.29 129.89 4.93 0.71 0.
(a)ImageNet-1k (256x256) Results with REPA.Adding∆FMto REPA
further improves SiT models across nearly all metrics, and by as much as
6.81 FID.
```
```
Metrics
Model FID↓ IS↑ sFID↓ Prec.↑ Rec.↑
REPA SiT-B/2 31.90 56.96 13.78 0.67 0.
+ Using∆FM 24.48 64.74 5.89 0.71 0.
REPA SiT-XL/2 11.32 119.72 10.21 0.76 0.
+ Using∆FM 7.64 131.50 4.72 0.79 0.
(b)ImageNet-1k (512x512) Results with REPA.∆FMis robust with
REPA at large image resolutions, further improving performance across
established metrics.
Table 2. REPA SiT [ 29 ] results on ImageNet-1k ( 256 × 256 ; a) and
( 512 × 512 ; b). All models are trained for 400K iterations strictly
following the procedure in [ 44 ], and setλ= 0. 05. We use the SDE
Euler-Maruyama sampler with NFE=50 without classifier guidance
for all our metrics.
```
```
Metric REPA-MMDiT
Flow-Matching ∆FM
FID↓ 24 19
Table 3.∆FMimproves on CC3M 256×256.We use the SDE
Euler-Maruyama sampler with NFE=50 without classifier-free guid-
ance.
```
```
tional costs during inference. In this section, we demonstrate
that when computational resources permit, combining∆FM
with CFG can yield further performance enhancements.
Accounting for conflicts.CFG and∆FMencourage flow
matching model generations to be unique and identifiable, in
different ways. Specifically,∆FMtrains models whose con-
ditional flows are steered away from other arbitrary flows in
the training data, regardless of generation state (xt). In con-
trast, CFG steers generations away from the unconditional
flow estimates based onxt. Thus, the signals from each
may not always be aligned and naively coupling them may
lead to conflicts and suboptimal generations. Fortunately,
we can quantify the amount of steerage∆FMapplies on
flow matching models by deriving the closed-form solution
to Eq. 4:minθL(∆FM)(θ) =
```
```
h
(minθL(FM)(θ))−λTˆ
```
```
i
/[1−λ],
whereTˆis simply themeanof all sample trajectories from
the training set (please see App. B.1 for the full deriva-
tion). Thus,∆FMyields models which estimate flows away
```

Figure 4.Contrastive flow-matching (∆FM) denoises significantly more efficiently than flow-matching.We visualize the expected final
image estimated by a flow-model when denoised every 5 steps for trajectories of length 30 steps using the SDE Euler-Maruyama sampler
and do not use classifier guidance. We compare the trajectories of a REPA SiT-XL/2 [ 44 ] trained on ImageNet-256 [ 10 ] for 400K steps with
flow-matching (FM), and the same model trained with the contrastive flow-matching (∆FM) objective. We show these trajectories in sets of
pairs generated from the same noise sample during inference, with the flow-matching model above our∆FM version.


```
Model CFG Terms Metric
w σlow σhigh IS↑ FID↓ sFID↓
REPA SiT-XL/2 1.75 0.0 0.75 280.33 2.09 5.
+ Using∆FM 1.85 0.0 0.65 281.95 1.97 4.
```
Table 4.ImageNet 256×256 Results with CFG and NFE=50.“w”
denotes the classifier-free guidance (CFG) weight, and[σlow,σhigh]
is the time interval under which CFG is applied. We report
the best results for each model after conducting a grid search
overw∈ { 1. 25 , 1. 75 , 1. 8 , 1. 85 , 2. 25 },σlow= 0andσhigh ∈
{ 0. 50 , 0. 65 , 0. 75 , 1. 0 }.∆FM outperforms FM on all metrics.

from thedata-drivenunconditional trajectory, weighted by
λ. While optimizer and training dynamics cannot guaran-
tee that all models trained with∆FMexactly decompose
into these terms,Tˆnevertheless approximates its effect on
these models. WithTˆ, we can account for conflicts be-
tween∆FMand CFG by modifying the CFG equation to:
CFGˆ = (1−λ) [wv(xt|y) + (1−w)v(xt|∅)]+λτ, wherew
is the guidance scale,∅is the unconditional term andλis the
same parameter used during∆FMtraining (Appendix B.
contains the full derivation). Note that, we only applyCFGˆ
within the specified guidance interval[σlow,σhigh], and use
ourunchanged∆FM model outside this interval.
Results. Table 4 summarizes the results. When paired
with CFG,∆FMimproves flow matching models across all
metrics, demonstrating its efficacy in settings where compu-
tational costs are not a constraint.

Additional Couplings. While we find that our proposed
coupling strategy for∆FMand CFG works well for our
setting, other suitable variations may also exist. For instance,
one may instead reduce conflicts by following the equation:
CFG ̃ = (w+λ)v(xt|y)−(1−w)v(xt|∅)−λTˆ, where
λ, andware free hyperparameters. We leave such explo-
ration to future work.

### 5.5. Analyzing Contrastive Flow-Matching

Understanding the∆FMweight (λ).λdirectly controls
how unique flows are across classes. Increasingλencour-
ages every diffusion step to be fully discriminative, enabling
models to encode distinct representations that integral to gen-
erating strong visual outputs at each trajectory step. How-
ever, setting it too high can lead to overly-separated flow
trajectories, making it difficult to capture the class structure
(Table 5.5). However,λvalues that are too low mirror the
flow matching objective. Notably, we find thatλ= 0. 05
is stable across all model and dataset settings, consistently
achieving strong performance.
Earlier class differentiation during denoising.In Figure 4,
we study flow trajectories of standard flow matching (FM)
and flow matching with∆FM. To do this, we take partially
denoised latents at various intermediate time steps along a

```
Metric ∆FMλValues
0. 0 0. 001 0. 01 0. 05 0. 1 0. 15
IS↑ 115.83 115.70 119.41 129.89 116.27 82.
FID↓ 11.14 10.93 9.93 7.29 9.86 19.
Table 5.λ= 0. 05 is ideal.We show an ablation of the∆FM
weight parameterλ. A too largeλproduces degenerate distribu-
tions that do not model class structure well. Too lowλis essen-
tially identical to flow-matching, with very little effect on training.
λ= 0. 05 is best and we use this for all our experiments.
```
```
Metrics
Model Batch Size FID↓ IS↑ sFID↓
REPA SiT-B/2 256 42.28 38.04 11.
+ Using∆FM 256 33.39 43.44 5.
REPA SiT-B/2 512 24.45 69.15 11.
+ Using∆FM 512 17.06 81.41 5.
REPA SiT-B/2 1024 22.00 76.15 11.
+ Using∆FM 1024 15.23 88.53 5.
REPA SiT-XL/2 256 11.14 115.83 8.
+ Using∆FM 256 7.29 129.89 4.
REPA SiT-XL/2 512 10.15 129.43 9.
+ Using∆FM 512 6.36 146.17 5.
Table 6.∆FMScales with Batch Size.We train all models for
400K iterations and strictly follow the protocol of [ 44 ]. All metrics
are measured with the SDE Euler-Maruyama sampler with NFE=
and without classifier guidance. We useλ= 0. 05 for all models
trained with∆FMand do not change any other hyperparameters.↑
indicates that higher values are better, with↓denoting the opposite.
Improvement using∆FMevenly scales with batch-size, and even
outperforms flow-matching models withhalfthe batch-size.
```
```
trajectory with total length 30. While initially both follow
similar trajectories, they quickly diverge within the first sev-
eral steps of the denoising process. For instance, the model
trained with∆FMproduces more structurally coherent im-
ages earlier (around 15 to 20 steps in) than with FM. The
iconic features of each class, such as slanted bridge surfaces
(Figure 4 (top-left)), animal eyes (Figure 4 (upper-left and
top-right), and train windows (Figure 4 (upper-right)), are
more clearly visible early on during the diffusion process of
the∆FMmodel. This enables∆FMto ultimately generate
higher quality images at the final timestep.
Effects of batch size on∆FM.In Table 5.5, we study the ef-
fects of batch size on our loss. It is well known that batch size
has an important effect on contrastive style losses [5, 7, 15]
that draw negatives within the batch. This can be under-
stood as a sample diversity issue. If the batch size is larger
than negative samples within the batch are more representa-
tive of the true distribution. In this table, we see a similar
```

```
FID
```
- 50K

```
100
```
```
10
0 50 200 250
Total Denoising Steps
```
```
Flow-Matching
Contrastive Flow-Matching
```
(^100150) ;
Training Iteration
50K 100K 1M 2M
9x Faster
200K
Flow-Matching
Contrastive Flow-Matching
FID

- 50K
    10

```
50
```
```
Figure 5.∆FMrequires significantly fewer training iterations
and inference-time denoising steps.We plot FID-50k on Ima-
geNet 256x256 with different numbers of training iterations and
denoising steps. We see that∆FMoutperforms the baseline with
9 ×fewertraining iterations and 5 ×reductionin the number of
inference-time denoising steps, indicating that∆FMis more effi-
cient in both training and inference.
```
trend: larger batch sizes are important for maximizing the
performance of∆FMacross several model scales. We also
maintain our improvements over the REPA baseline through
all batch sizes and model scales.
Improved training and inference speed.In Figure 5 (left),
we see the significant improvements in training speed from
the∆FMobjective. We reach the same performance (mea-
sured by FID-50k) as baseline with 9 ×fewer training iter-
ations. In Figure 5 (right), we also demonstrate significant
improvements at inference time. With our objective, we
reach superior performance with only 50 denoising steps
compared to the baseline with 250 denoising steps. This is a
linear 5×improvement in training efficiency. Taken together,
these results emphasize the important gains in computational
efficiency achieved by our method.

## 6. Conclusion

We introduced Contrastive Flow Matching (∆FM), a simple
addition to the diffusion objective that enforces distinct,
diverse flows during image generation. Quantitatively,∆FM
results in improved image quality with far fewer denoising
steps ( 5 ×faster) and significantly improved training speed
( 9 ×faster). Qualitatively,∆FMimproves the structural
coherence and global semantics for image generation.
All of this is achieved with negligible extra compute per
training iteration. Finally, we show that our improvements
stack with the recently proposed Representation Alignment
(REPA) loss, allowing for strong gains in image generation
performance. Looking forward,∆FMshows the possibility
that deviating from perfect distribution modeling in the
diffusion objective might result in better image generation.

## References

```
[1]Michael S Albergo and Eric Vanden-Eijnden. Building nor-
malizing flows with stochastic interpolants.arXiv preprint
arXiv:2209.15571, 2022. 3
```
```
[2]Michael S Albergo, Nicholas M Boffi, and Eric Vanden-
Eijnden. Stochastic interpolants: A unifying framework for
flows and diffusions.arXiv preprint arXiv:2303.08797, 2023.
3
[3]Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,
Hang Su, and Jun Zhu. All are worth words: A vit backbone
for diffusion models. InCVPR, 2023. 5
[4]Gongze Cao, Yezhou Yang, Jie Lei, Cheng Jin, Yang Liu, and
Mingli Song. Tripletgan: Training generative model with
triplet loss, 2017. 3
[5]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. InICCV,
```
2021. 2, 5, 6, 8
[6]Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and
David Duvenaud. Neural ordinary differential equations. In
Advances in Neural Information Processing Systems, 2018. 3
[7]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-
frey Hinton. A simple framework for contrastive learning of
visual representations.ICLR, 2020. 8
[8]Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L
Klasky, and Jong Chul Ye. Diffusion posterior sam-
pling for general noisy inverse problems. arXiv preprint
arXiv:2209.14687, 2022. 3
[9]Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin
Nam, and Jong Chul Ye. Cfg++: Manifold-constrained clas-
sifier free guidance for diffusion models. arXiv preprint
arXiv:2406.08070, 2024. 3
[10]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009. 5, 7
[11]Alakh Desai and Nuno Vasconcelos. Improving image syn-
thesis with diffusion-negative sampling, 2024. 2
[12]Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis.Advances in neural information
processing systems, 34:8780–8794, 2021. 3, 5
[13]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. InICLR, 2021. 6
[14]Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-
tezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, ̈
Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn,
Zion English, and Robin Rombach. Scaling rectified flow
transformers for high-resolution image synthesis.ICML, 2024.
2, 6
[15]Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual repre-
sentation learning.CVPR, 2020. 8
[16]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium.
Advances in neural information processing systems, 30, 2017.
5


[17]Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance.arXiv preprint arXiv:2207.12598, 2022. 3
[18]Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance, 2022. 2
[19]Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. InAdvances in Neural Information
Processing Systems, 2020. 3
[20]Tero Karras, Miika Aittala, Tuomas Kynka ̈anniemi, Jaakko ̈
Lehtinen, Timo Aila, and Samuli Laine. Guiding a dif-
fusion model with a bad version of itself. arXiv preprint
arXiv:2406.02507, 2024. 2, 3
[21]Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas De-
meester, and Luca Ambrogioni. Dynamic negative guidance
of diffusion models: Towards immediate content removal. In
Neurips Safe Generative AI Workshop 2024. 3
[22]Tuomas Kynk ̈a ̈anniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and recall
metric for assessing generative models.NeurIPS, 2019. 5
[23]Tuomas Kynk ̈a ̈anniemi, Miika Aittala, Tero Karras, Samuli
Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance
in a limited interval improves sample and distribution quality
in diffusion models.arXiv preprint arXiv:2404.07724, 2024.
3
[24]Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximil-
ian Nickel, and Matthew Le. Flow matching for generative
modeling. InICLR, 2023. 3
[25]Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximil-
ian Nickel, and Matthew Le. Flow matching for generative
modeling. InICLR, 2023. 1, 3, 4
[26]Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight
and fast: Learning to generate and transfer data with rectified
flow, 2022. 3
[27]Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan
Li, and Jun Zhu. Contrastive energy prediction for exact
energy-guided diffusion sampling in offline reinforcement
learning. InInternational Conference on Machine Learning,
pages 22825–22855. PMLR, 2023. 3
[28]Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang
Zhao. Latent consistency models: Synthesizing high-
resolution images with few-step inference. arXiv preprint
arXiv:2310.04378, 2023. 3
[29]Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M.
Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring
flow and diffusion-based generative models with scalable
interpolant transformers. 2024. 1, 2, 3, 5, 6
[30]Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W
Battaglia. Generating images with sparse representations.
arXiv preprint arXiv:2103.03841, 2021. 5
[31]Maxime Oquab, Timoth ́ee Darcet, Th ́eo Moutakanni, Huy V.
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby,
Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell
Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael
Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Je-
gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr
Bojanowski. DINOv2: Learning robust visual features with-
out supervision.TMLR, 2024. 2

```
[32]Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bjorn Ommer. High-resolution image ̈
synthesis with latent diffusion models. InProceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022. 5
[33]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. Imagenet large scale visual recognition challenge,
```
2015. 4
[34]Tim Salimans and Jonathan Ho. Progressive distillation
for fast sampling of diffusion models. arXiv preprint
arXiv:2202.00512, 2022. 3
[35]Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans.Advances in neural information processing
systems, 29, 2016. 5
[36]Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clus-
tering. In2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), page 815–823. IEEE, 2015. 3
[37]Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, image
alt-text dataset for automatic image captioning. InProceed-
ings of ACL, 2018. 2, 5, 6
[38]Rahul Shenoy, Zhihong Pan, Kaushik Balakrishnan, Qisen
Cheng, Yongmoon Jeon, Heejune Yang, and Jaewon Kim.
Gradient-free classifier guidance for diffusion model sam-
pling.arXiv preprint arXiv:2411.15393, 2024. 3
[39]Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. InInternational Conference on
Learning Representations, 2021. 3
[40]Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mar-
dani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash
Vahdat. Loss-guided diffusion models for plug-and-play con-
trollable generation. InInternational Conference on Machine
Learning, pages 32483–32498. PMLR, 2023. 3
[41]Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based
generative modeling in latent space, 2021. 3
[42]Tianwei Yin, Micha ̈el Gharbi, Taesung Park, Richard Zhang,
Eli Shechtman, Fredo Durand, and William T Freeman. Im-
proved distribution matching distillation for fast image syn-
thesis.arXiv preprint arXiv:2405.14867, 2024. 3
[43]Tianwei Yin, Micha ̈el Gharbi, Richard Zhang, Eli Shecht-
man, Fredo Durand, William T Freeman, and Taesung Park.
One-step diffusion with distribution matching distillation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 6613–6623, 2024. 3
[44]Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong,
Jonathan Huang, Jinwoo Shin, and Saining Xie. Representa-
tion alignment for generation: Training diffusion transformers
is easier than you think, 2024. 1, 2, 3, 5, 6, 7, 8
[45]Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Un-
paired image-to-image translation via energy-guided stochas-
tic differential equations.Advances in Neural Information
Processing Systems, 35:3609–3623, 2022. 3


[46]Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai
Huang. Long and short guidance in score identity distilla-
tion for one-step text-to-image generation. arXiv preprint
arXiv:2406.01561, 2024. 3


## A. Text-to-Image Qualitative Results

We visualize generations between our REPA-MMDiT mod-
els described in Section 5.3 trained with flow-matching (FM)
loss and with∆FMon CC3M with a batch size of 256 for
400K iterations in Figure 6. We plot images in pairs, with
FM images on the left and∆FMimages on the right, and
show the respective caption for each pair above. All im-
ages are generated without classifier-free guidance and using
NFE=50, and are the same images used in Table 3.

## B. Deriving Contrastive-Flow Matching Inter-

## ference

### B.1. Closed-form solution to Eq. 4

We first re-introduce Eq. 4 for convenience,

```
L(∆FM)(θ) = E
```
#### "

```
||vθ(xt,t,y)−( ̇αtˆx+ ̇σtε)||^2
−λ||vθ(xt,t,y)−( ̇αt ̃x+ ̇σt ̃ε)||^2
```
#### #

```
Minimizing the expectation, expanding all norms and letting
v(θ) =v(xt,t,y), we can simplify the expectation to:
```
```
= min
θ
```
#### E

#### 

#### 

#### 

#### 

#### 

```
(1−λ)v(θ)Tv(θ)
− 2 v(θ)T[( ̇αtˆx+ ̇σtε)−λ( ̇αtx ̃+ ̇σt ̃ε)]
+( ̇αtxˆ+ ̇σtε)T( ̇αtxˆ+ ̇σtε)
−λ( ̇αtx ̃+ ̇σt ̃ε)T( ̇αtx ̃+ ̇σt ̃ε)
```
#### 

#### 

#### 

#### 

#### 

#### (7)

```
= min
θ
```
#### E

#### "

```
(1−λ)v(θ)Tv(θ)
− 2 v(θ)T[( ̇αtˆx+ ̇σtε)−λ( ̇αt ̃x+ ̇σt ̃ε)]
```
#### #

#### (8)

```
∝∼min
θ
```
#### E

#### 

#### 

#### 

#### √

```
1 −λv(θ)
```
```
−
```
```
( ̇αtˆx+ ̇σtε)−λ( ̇αt ̃x+ ̇σt ̃ε)
√
1 −λ
```
```
2
```
```
2
```
#### 

#### 

####  (9)

```
Setting the gradient with respect tov(θ)to 0 ,
```
```
√
1 −λv(θ)∗= E
```
#### 

```
( ̇αtˆx+ ̇σtε)−λ( ̇αtx ̃+ ̇σt ̃ε)
√
1 −λ
```
#### 

#### (10)

```
v(θ)∗=
```
```
E [ ̇αtxˆ+ ̇σtε]−λE [ ̇αtx ̃+ ̇σt ̃ε]
1 −λ
```
#### (11)

```
Finally, observe thatE [ ̇αtxˆ+ ̇σtε]is the solution to the
flow-matching objective. SettingE [ ̇αtx ̃+ ̇σt ̃ε] =Tˆand
observing thatxtdoes not depend onxˆorˆεwe obtain:
```
```
min
θ
L(∆FM)(θ) =
```
```
minθL(FM)(θ)−λTˆ
1 −λ
```
#### (12)

### B.2. Coupling with CFG

```
Classifier-free guidance (CFG) is originally defined over the
flow-matching solution ofminθL(FM). Re-writing Eq. 12
```
```
and substituting it into the CFG equation, we obtain:
```
```
CFG=wv(FM)(xt,t,y) + (1−w)v(FM)(xt,t,∅)(13)
```
#### =

#### 

#### 

#### 

```
w
```
```
h
(1−λ)v(∆FM)(xt,t,y) +λTˆ
```
```
i
```
```
−(1−w)
```
```
h
(1−λ)v(∆FM)(xt,t,∅) +λTˆ
```
```
i
```
#### 

#### 

####  (14)

#### =

#### "

```
(1−λ)
```
#### "

```
wv(∆FM)(xt,t,y)
+(1−w)v(∆FM)(xt,t,∅)
```
#### #

```
+λTˆ
```
#### #

#### (15)

```
Letting v(xt|y) = v(∆FM)(xt,t,y) and v(xt|∅) =
v(∆FM)(xt,t,∅), we obtain the Eq. from Section 5.4:
CFGˆ = (1−λ) [wv(xt|y) + (1−w)v(xt|∅)] +λTˆ.
```

Figure 6.CC3M side-by-side generations between a REPA-MMDiT model trained with flow-matching (left) and∆FM(right).
Models are trained for 400K iterations using a batch-size of 256 and images are generated without classifier-free guidance and using
NFE=50.


